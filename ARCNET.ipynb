{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5596337",
   "metadata": {},
   "source": [
    "# ARCNET\n",
    "***\n",
    "## TODO:\n",
    "\n",
    "- **Transfer class functions of `ARCNET.models.arcnet.ConceptModule` to their respective locations**\n",
    "    - e.g., *assembly_complexity* / *mutation* / *message_passing* should go into core / evolution / etc. \n",
    "    - update ConceptModule with gettr settr methods to call replaced methods\n",
    "<br>\n",
    "- **Clean up `ARCNET.core.trainer` and `ARCNET.models.arcnet.ConceptModule` and `ARCNET.evolution.fitness`**\n",
    "    - bloat comments and legacy code\n",
    "    - ensure loops are working as intended\n",
    "<br>\n",
    "- **Implement a simple assembly tracking to ensure that it is properly working**\n",
    "    -It's hard to tell with so many interconnected parts that if assembly tracking is properly happening. Some testruns need to be done with early stop output to see how hashes of layer weights are being stored and transfered. \n",
    "    - Implement testing code for above statement \n",
    "<br>\n",
    "- **Update output functions for graphical viewing**\n",
    "    - code was copy/paste from original 8000 line jupyter notebook and not implemented properly in the new modular system, needs to be\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a3741",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "***\n",
    "## Methods:\n",
    "\n",
    "#### 1. State Space Definition\n",
    "Let $\\mathcal{S}$ be the system state space where each module $m_i^{(t)}$ at time $t$ is defined as:\n",
    "\n",
    "$$m_i^{(t)} = \\{\\mathbf{W}_i, \\mathbf{b}_i, \\mathbf{p}_i^{(t)}, Q_i^{(t)}, f_i^{(t)}, A_i^{(t)}\\}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{W}_i, \\mathbf{b}_i$: Neural network parameters  \n",
    "- $\\mathbf{p}_i^{(t)} \\in [0,1]^d$: Position on learned manifold $\\mathcal{M}$  \n",
    "- $Q_i^{(t)}$: Q-learning function (neural or tabular)  \n",
    "- $f_i^{(t)} \\in [0,1]$: Fitness score  \n",
    "- $A_i^{(t)}$: Assembly properties  \n",
    "\n",
    "#### 2. Core Mathematical Operations\n",
    "\n",
    "**2.1 Manifold Learning Component**\n",
    "\n",
    "The system learns a manifold embedding: \n",
    "\n",
    "$$\n",
    "\\mathbf{z}_i = \\text{Encoder}(\\bar{\\mathbf{x}}) \\rightarrow \\mathbf{p}_i^{(0)} = \\sigma(\\mathbf{z}_i)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\mathbf{z}_i$: Latent vector for module $i$  \n",
    "- $\\text{Encoder}(\\cdot)$: Neural encoder function  \n",
    "- $\\bar{\\mathbf{x}}$: Input data  \n",
    "- $\\sigma(\\cdot)$: Activation function (e.g., sigmoid)  \n",
    "- $\\mathbf{p}_i^{(0)}$: Initial position on manifold for module $i$  \n",
    "\n",
    "Geodesic distance on learned manifold: \n",
    "\n",
    "$$\n",
    "d_{\\mathcal{M}}(\\mathbf{p}_i, \\mathbf{p}_j) = \n",
    "\\begin{cases} \n",
    "|\\mathbf{p}_i - \\mathbf{p}_j|_2 & \\text{if } \\mathbf{T}_i = \\emptyset \\\\\n",
    "|\\mathbf{T}_i^T(\\mathbf{p}_j - \\mathbf{p}_i)|_2 \\cdot (1 + 0.1|\\kappa_i||\\mathbf{T}_i^T(\\mathbf{p}_j - \\mathbf{p}_i)|_2) & \\text{otherwise} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $d_{\\mathcal{M}}(\\mathbf{p}_i, \\mathbf{p}_j)$: Geodesic distance between modules $i$ and $j$  \n",
    "- $\\mathbf{p}_i, \\mathbf{p}_j$: Positions on manifold  \n",
    "- $\\mathbf{T}_i$: Tangent space at $\\mathbf{p}_i$  \n",
    "- $\\kappa_i$: Curvature at $\\mathbf{p}_i$  \n",
    "- $|\\cdot|_2$: Euclidean norm  \n",
    "\n",
    "**2.2 Q-Learning Evolution Dynamics**\n",
    "\n",
    "Each module maintains a Q-function $Q_i: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ updated via: \n",
    "\n",
    "$$\n",
    "Q_i(s,a) \\leftarrow Q_i(s,a) + \\alpha[R(m_{\\text{child}}) + \\gamma \\max_{a'} Q_i(s',a') - Q_i(s,a)]\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $Q_i(s,a)$: Q-value for state $s$ and action $a$ for module $i$  \n",
    "- $\\alpha$: Learning rate  \n",
    "- $R(m_{\\text{child}})$: Reward for child module  \n",
    "- $\\gamma$: Discount factor  \n",
    "- $s'$: Next state  \n",
    "- $a'$: Next action  \n",
    "- $\\max_{a'} Q_i(s',a')$: Maximum Q-value for next state  \n",
    "\n",
    "Proof of Q-Learning Convergence: Under standard assumptions (bounded rewards, sufficient exploration), this satisfies the Robbins-Monro conditions, guaranteeing convergence to $Q^*$.\n",
    "\n",
    "**2.3 Multi-Objective Fitness Function**\n",
    "\n",
    "The adaptive fitness function implements a time-varying optimization:\n",
    "\n",
    "$$\n",
    "F_i^{(t)} = w_1^{(t)} \\cdot \\text{Accuracy}_i + w_2^{(t)} \\cdot \\text{Diversity}_i + w_3^{(t)} \\cdot \\text{Entropy}_i - \\text{Penalty}_i^{(t)}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $F_i^{(t)}$: Fitness of module $i$ at time $t$  \n",
    "- $w_1^{(t)}, w_2^{(t)}, w_3^{(t)}$: Time-varying weights  \n",
    "- $\\text{Accuracy}_i$: Accuracy metric for module $i$  \n",
    "- $\\text{Diversity}_i$: Diversity metric  \n",
    "- $\\text{Entropy}_i$: Entropy metric  \n",
    "- $\\text{Penalty}_i^{(t)}$: Penalty term at time $t$  \n",
    "\n",
    "Weights evolve as: \n",
    "\n",
    "$$\n",
    "w^{(t)} = \n",
    "\\begin{cases} \n",
    "(1.5w_n, 1.2w_e, 0.8w_a) & \\text{if } t < 0.3T \\\\\n",
    "(w_n, w_e, w_a) & \\text{if } 0.3T \\leq t < 0.7T \\\\\n",
    "(0.7w_n, 0.8w_e, 1.3w_a) & \\text{if } t \\geq 0.7T \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $w_n, w_e, w_a$: Base weights for accuracy, diversity, and entropy  \n",
    "- $T$: Total time steps  \n",
    "- $t$: Current time step  \n",
    "\n",
    "#### 3. Autocatalytic Assembly Dynamics\n",
    "\n",
    "The assembly complexity grows according to: \n",
    "\n",
    "$$\n",
    "A_{\\text{sys}}^{(t)} = \\frac{1}{|\\mathcal{P}|} \\sum_{i=1}^{|\\mathcal{P}|} e^{a_i} \\cdot \\frac{n_i - 1}{|\\mathcal{P}|}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $A_{\\text{sys}}^{(t)}$: System assembly complexity at time $t$  \n",
    "- $|\\mathcal{P}|$: Population size  \n",
    "- $a_i$: Assembly complexity of module $i$  \n",
    "- $n_i$: Copy number of module type $i$  \n",
    "\n",
    "Assembly Theorem: Under controlled catalysis, the system complexity $A_{\\text{sys}}^{(t)}$ is bounded by: \n",
    "\n",
    "$$\n",
    "A_{\\text{sys}}^{(t)} \\leq C \\cdot \\log(t) + A_0\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $C$: Constant  \n",
    "- $A_0$: Initial complexity  \n",
    "- $t$: Time step  \n",
    "\n",
    "Proof Sketch: The exponential assembly growth is constrained by the bias elimination mechanism and survivor selection, creating a logarithmic upper bound.\n",
    "\n",
    "#### 4. Bias Elimination Mechanism\n",
    "\n",
    "The system implements an adaptive bias detection function: \n",
    "\n",
    "$$\n",
    "\\text{Bias}(m_i, t) = \\max_k P(y_i = k | \\mathbf{x}) - \\text{threshold}(t)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\text{Bias}(m_i, t)$: Bias for module $i$ at time $t$  \n",
    "- $P(y_i = k | \\mathbf{x})$: Probability of class $k$ given input $\\mathbf{x}$  \n",
    "- $\\text{threshold}(t)$: Adaptive threshold at time $t$  \n",
    "\n",
    "Where $\\text{threshold}(t) = 0.75 + 0.2 \\cdot \\frac{t}{T}$ creates an adaptive tolerance.\n",
    "\n",
    "- $T$: Total time steps  \n",
    "- $t$: Current time step  \n",
    "\n",
    "Anti-Convergence Theorem: The bias elimination ensures population diversity: \n",
    "\n",
    "$$\n",
    "\\lim_{t \\rightarrow \\infty} \\mathbb{E}[\\text{Entropy}(\\mathcal{P}^{(t)})] \\geq H_{\\min} > 0\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\mathbb{E}[\\text{Entropy}(\\mathcal{P}^{(t)})]$: Expected entropy of population at time $t$  \n",
    "- $H_{\\min}$: Minimum entropy bound  \n",
    "\n",
    "#### 5. Message Passing and Information Flow\n",
    "\n",
    "For each module $m_i$, select neighbors by manifold distance: \n",
    "\n",
    "$$\n",
    "\\mathcal{N}_{\\mathcal{M}}(m_i) = \\{m_j : d_{\\mathcal{M}}(\\mathbf{p}_i, \\mathbf{p}_j) \\text{ among } k \\text{ smallest}\\}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\mathcal{N}_{\\mathcal{M}}(m_i)$: Set of $k$ nearest neighbors to $m_i$ on the manifold  \n",
    "- $d_{\\mathcal{M}}(\\mathbf{p}_i, \\mathbf{p}_j)$: Manifold distance between $i$ and $j$  \n",
    "- $k$: Number of neighbors  \n",
    "\n",
    "Information propagates via manifold-aware messaging: \n",
    "\n",
    "$$\n",
    "\\mathbf{M}_i^{(t)} = \\sigma(g_i) \\cdot \\frac{1}{|\\mathcal{N}_{\\mathcal{M}}(m_i)|} \\sum_{j \\in \\mathcal{N}_{\\mathcal{M}}(m_i)} \\frac{\\mathbf{h}_j^{(t-1)}}{1 + d_{\\mathcal{M}}(\\mathbf{p}_i, \\mathbf{p}_j)}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\mathbf{M}_i^{(t)}$: Message received by module $i$ at time $t$  \n",
    "- $\\sigma(g_i)$: Activation function applied to gating variable $g_i$  \n",
    "- $|\\mathcal{N}_{\\mathcal{M}}(m_i)|$: Number of neighbors  \n",
    "- $\\mathbf{h}_j^{(t-1)}$: Hidden state of neighbor $j$ at previous time  \n",
    "- $d_{\\mathcal{M}}(\\mathbf{p}_i, \\mathbf{p}_j)$: Manifold distance  \n",
    "\n",
    "Information Flow Theorem: Under connected manifold topology, information propagates globally in $O(\\log n)$ steps.\n",
    "\n",
    "#### 6. Main Convergence Proof\n",
    "\n",
    "Theorem (AAN-Q Global Convergence): Under the following conditions:\n",
    "\n",
    "- Bounded fitness landscape: $F: \\mathcal{S} \\rightarrow [0,1]$\n",
    "- Sufficient exploration: $\\epsilon$-greedy Q-learning with $\\epsilon > 0$\n",
    "- Controlled bias elimination: $|\\text{eliminated}| \\leq \\alpha |\\mathcal{P}|$ per step\n",
    "- Manifold Lipschitz continuity: $|F(\\mathbf{p}_1) - F(\\mathbf{p}_2)| \\leq L \\cdot d_{\\mathcal{M}}(\\mathbf{p}_1, \\mathbf{p}_2)$\n",
    "\n",
    "The system converges to a stable configuration: \n",
    "\n",
    "$$\n",
    "\\lim_{t \\rightarrow \\infty} \\mathbb{E}[F_{\\text{best}}^{(t)}] = F^* - \\delta\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\mathbb{E}[F_{\\text{best}}^{(t)}]$: Expected best fitness at time $t$  \n",
    "- $F^*$: Optimal fitness  \n",
    "- $\\delta$: Exploration-exploitation gap  \n",
    "\n",
    "Proof Strategy:\n",
    "\n",
    "- Q-Learning Convergence: Standard MDP theory guarantees $Q_i \\rightarrow Q_i^*$\n",
    "- Manifold Regularization: The geodesic distance creates smooth fitness landscapes\n",
    "- Population Dynamics: Bias elimination prevents premature convergence\n",
    "- Assembly Constraints: Bounded complexity prevents runaway growth\n",
    "\n",
    "#### 7. Computational Complexity\n",
    "\n",
    "The algorithm has complexity:\n",
    "\n",
    "Per Step: $O(n^2 d + n \\cdot |Q|)$ where $n = |\\mathcal{P}|$, $d$ = manifold dimension\n",
    "\n",
    "Overall: $O(T \\cdot n^2 d)$ for $T$ evolution steps\n",
    "\n",
    "Where:  \n",
    "- $n$: Population size  \n",
    "- $d$: Manifold dimension  \n",
    "- $|Q|$: Size of Q-table or Q-function  \n",
    "- $T$: Number of evolution steps  \n",
    "\n",
    "#### 8. System Complexity\n",
    "\n",
    "**8.1 Assembly Theory Metric**\n",
    "\n",
    "$$\n",
    "A_{\\text{sys}}^{(t)} = \\frac{1}{|\\mathcal{P}|} \\sum_{i=1}^{|\\mathcal{P}|} e^{a_i} \\cdot \\frac{n_i - 1}{|\\mathcal{P}|}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $A_{\\text{sys}}^{(t)}$: System assembly complexity at time $t$  \n",
    "- $|\\mathcal{P}|$: Population size  \n",
    "- $a_i$: Assembly complexity of module $i$  \n",
    "- $n_i$: Copy number of module type $i$  \n",
    "\n",
    "Each neural module (`ConceptModule`) tracks the minimal assembly complexity of it's weights. \n",
    "- Each layer's weights are wrapped in `ARCENET.core.ModuleComponent` that records the pathway\n",
    "- When a module is mutated, new components are created with parent references, enabling reuse tracking\n",
    "- The per-layer and total assemble complexity can be queried for any module with `ARCENET.models.arcnet_learner.system_assembly_complexity`\n",
    "- System-level complexity is computed as the formula above, using the sum of exponentiated assembly indicies\n",
    "\n",
    "#### 9. Complete System Evolution\n",
    "\n",
    "The system evolves according to: \n",
    "\n",
    "$$\n",
    "\\mathcal{P}^{(t+1)} = \\text{Select}(\\mathcal{P}^{(t)}) \\cup \\text{Mutate}(\\text{Select}(\\mathcal{P}^{(t)}), Q^{(t)})\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\mathcal{P}^{(t)}$: Population at time $t$  \n",
    "- $\\text{Select}(\\cdot)$: Selection operator  \n",
    "- $\\text{Mutate}(\\cdot, Q^{(t)})$: Mutation operator using Q-function $Q^{(t)}$  \n",
    "\n",
    "With the objective of maximizing: \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{t,i}\\left[ R(m_i^{(t)}) \\right] - \\lambda A_{\\text{sys}}^{(t)}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\mathcal{L}$: Objective function  \n",
    "- $R(m_i^{(t)})$: Reward for module $i$ at time $t$  \n",
    "- $\\lambda$: Regularization parameter  \n",
    "- $A_{\\text{sys}}^{(t)}$: System assembly complexity at time $t$  \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe6c854",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63977cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# import ARCNET modules\n",
    "#from core.trainer import Trainer\n",
    "\n",
    "# set all seeds\n",
    "seeds = 42\n",
    "random.seed(seeds)\n",
    "np.random.seed(seeds)\n",
    "torch.manual_seed(seeds)\n",
    "torch.cuda.manual_seed_all(seeds)\n",
    "\n",
    "# suppress UserWarnings from torchvision (no User path output)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torchvision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07672bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA version: 12.8\n",
      "cuDNN version: 90701\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Total GPU memory: 8.00 GB\n",
      "Reserved by PyTorch: 0.00 GB\n",
      "Currently allocated: 0.00 GB\n",
      "Free inside reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())  # Should print True if a compatible GPU is available\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n",
    "print(torch.cuda.get_device_name(0))  # Name of the first GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    free = reserved - allocated\n",
    "    print(f\"Total GPU memory: {total:.2f} GB\")\n",
    "    print(f\"Reserved by PyTorch: {reserved:.2f} GB\")\n",
    "    print(f\"Currently allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Free inside reserved: {free:.2f} GB\")\n",
    "\n",
    "    # choose to enable device\n",
    "    global device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    global cpu\n",
    "    cpu = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb9181d",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Load Datasets & Split/Encode/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ea24081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to load the breast cancer dataset and split it into training and testing sets\n",
    "def load_breast_cancer_data(test_size=0.2, random_state=42):\n",
    "    \n",
    "    \"\"\"\n",
    "    default test_size is 0.2 (20% of the data for testing)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the breast cancer dataset\n",
    "    data = load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Output\n",
    "X_train, X_test, y_train, y_test = load_breast_cancer_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to load the wine dataset and split it into training and testing sets\n",
    "def load_wine_data(test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    default test_size is 0.2 (20% of the data for testing)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the wine dataset\n",
    "    data = fetch_openml(name='wine', version=1, as_frame=True)\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Encode the labels if necessary\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8456773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to load the MNIST dataset and split it into training and testing sets\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# resize and normalize\n",
    "x_train = np.reshape(x_train, [-1, 784])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = np.reshape(x_test, [-1, 784])\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30690633",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163df87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8279d7a7151a44f196b5a9f31a6439b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred weights for layer ea180341-98c5-4417-8143-038bfcd351a1\n",
      "Epoch 0 Loss: 1.0006 | Action: add\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from core.blueprint import ArchitectureBlueprint\n",
    "from core.registry import ComponentRegistry\n",
    "from evolution.mutation import mutate_blueprint\n",
    "from evolution.controller import MutationRLController\n",
    "from evolution.transfer import transfer_weights\n",
    "from models.multimodal_learner import MultimodalLearner\n",
    "from core.components import TrackedLayer\n",
    "from data.loader import CombinedDataset\n",
    "from data.image_adapter import ImageAdapter\n",
    "from data.text_adapter import TextAdapterDistilbert\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "mnist_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "dataset = CombinedDataset(mnist_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "blueprint = ArchitectureBlueprint()\n",
    "registry = ComponentRegistry()\n",
    "controller = MutationRLController()\n",
    "\n",
    "init_layer = TrackedLayer(128, 64)\n",
    "blueprint.add_module(init_layer)\n",
    "registry.register(init_layer)\n",
    "\n",
    "adapters = {'image': ImageAdapter(), 'text': TextAdapterDistilbert()}\n",
    "model = MultimodalLearner(blueprint, adapters)\n",
    "#model = model.to(device)\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    total_loss = 0\n",
    "    progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "    for batch in progress:\n",
    "        x, target = batch\n",
    "        output = model(x)\n",
    "        loss = torch.nn.functional.mse_loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    state = controller.get_state(loss_history, blueprint)\n",
    "    action = controller.select_action(state)\n",
    "    mutate_blueprint(blueprint, registry)\n",
    "    new_model = MultimodalLearner(blueprint, adapters)\n",
    "    transfer_weights(model, new_model)\n",
    "    model = new_model\n",
    "    next_state = controller.get_state(loss_history, blueprint)\n",
    "    controller.update(state, action, -avg_loss, next_state)\n",
    "    print(f\"Epoch {epoch} Loss: {avg_loss:.4f} | Action: {action}\")\n",
    "\n",
    "    blueprint.visualize(filename=f\"architecture_epoch_{epoch}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22cdb047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Stats:\n",
      "MultimodalLearner(\n",
      "  (fusion): CrossModalFusion(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (core): ModularGeneralLearner(\n",
      "    (layers): ModuleList(\n",
      "      (0): TrackedLayer(\n",
      "        (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total Parameters: 90816\n",
      "Final Average Loss: 1.0006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.50      0.05         2\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         2\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.00      0.00      0.00         2\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       0.33      1.00      0.50         1\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.00      0.00      0.00         1\n",
      "          18       0.00      0.00      0.00         6\n",
      "          19       0.00      0.00      0.00         2\n",
      "          20       0.00      0.00      0.00         3\n",
      "          21       0.00      0.00      0.00         2\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.00      0.00      0.00         1\n",
      "          26       0.00      0.00      0.00         2\n",
      "          27       0.00      0.00      0.00         3\n",
      "          28       0.00      0.00      0.00         2\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         6\n",
      "          32       0.00      0.00      0.00         1\n",
      "          35       0.00      0.00      0.00         1\n",
      "          36       0.00      0.00      0.00         2\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.00      0.00      0.00         1\n",
      "          42       0.00      0.00      0.00         1\n",
      "          43       0.00      0.00      0.00         1\n",
      "          44       0.00      0.00      0.00         1\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.00      0.00      0.00         2\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         0\n",
      "          49       0.00      0.00      0.00         2\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         3\n",
      "          53       0.00      0.00      0.00         2\n",
      "          54       0.00      0.00      0.00         0\n",
      "          56       0.00      0.00      0.00         1\n",
      "          58       0.00      0.00      0.00         2\n",
      "          59       0.00      0.00      0.00         1\n",
      "          60       0.00      0.00      0.00         2\n",
      "          61       0.00      0.00      0.00         2\n",
      "          62       0.00      0.00      0.00         2\n",
      "          63       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.02        96\n",
      "   macro avg       0.01      0.03      0.01        96\n",
      "weighted avg       0.00      0.02      0.01        96\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming output and target are tensors, move to cpu and convert to numpy\n",
    "y_true = target.cpu().detach().numpy()\n",
    "y_pred = output.cpu().detach().numpy()\n",
    "\n",
    "# If this is a classification task, get predicted classes\n",
    "if y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "    y_pred_classes = y_pred.argmax(axis=1)\n",
    "    y_true_classes = y_true.argmax(axis=1) if y_true.ndim > 1 and y_true.shape[1] > 1 else y_true\n",
    "else:\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "    y_true_classes = y_true.astype(int)\n",
    "\n",
    "print(\"Final Model Stats:\")\n",
    "print(model)\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Final Average Loss: {avg_loss:.4f}\")\n",
    "print(classification_report(y_true_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ef804dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All components in the assembly:\n",
      "TrackedLayer(\n",
      "  (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"All components in the assembly:\")\n",
    "for component in blueprint.modules:\n",
    "    print(component)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
